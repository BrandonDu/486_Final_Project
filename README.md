@ -1,547 +0,0 @@
[![pypi](https://img.shields.io/pypi/v/spflow.svg)](https://pypi.org/project/spflow/)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Build Status](https://travis-ci.com/SPFlow/SPFlow.svg?branch=master)](https://travis-ci.com/SPFlow/SPFlow)


# Ruben Carpenter, Brandon Du, Ayush Tibrewal 486 Final Project: Sum-Product Networks


## Getting Started

These instructions will get you a copy of the project up and running on your local machine for development and testing purposes.

## Examples

We start by creating an SPN. Using a Domain-Specific Language, we can create an SPN of categorical
leave nodes like this:


```python
from spn.structure.leaves.parametric.Parametric import Categorical

spn = 0.33 * (Categorical(p=[0.3, 0.7], scope=0) *
             (0.8 * (Categorical(p=[0.55, 0.45], scope=1) *
                     Categorical(p=[0.4, 0.6], scope=2))
            + 0.2 * (Categorical(p=[0.5, 0.5], scope=1) *
                     Categorical(p=[0.6, 0.4], scope=2)))) \
    + 0.67 * (Categorical(p=[0.25, 0.75], scope=0) *
             Categorical(p=[0.9, 0.1], scope=1) *
             Categorical(p=[0.4, 0.6], scope=2))
```

We can create the same SPN using the object hierarchy:

```python
from spn.structure.leaves.parametric.Parametric import Categorical

from spn.structure.Base import Sum, Product

from spn.structure.Base import assign_ids, rebuild_scopes_bottom_up


p0 = Product(children=[Categorical(p=[0.55, 0.45], scope=1), Categorical(p=[0.4, 0.6], scope=2)])
p1 = Product(children=[Categorical(p=[0.5, 0.5], scope=1), Categorical(p=[0.6, 0.4], scope=2)])
s1 = Sum(weights=[0.8, 0.2], children=[p0, p1])
p2 = Product(children=[Categorical(p=[0.3, 0.7], scope=0), s1])
p3 = Product(children=[Categorical(p=[0.25, 0.75], scope=0), Categorical(p=[0.9, 0.1], scope=1)])
p4 = Product(children=[p3, Categorical(p=[0.4, 0.6], scope=2)])
spn = Sum(weights=[0.33, 0.67], children=[p2, p4])

assign_ids(spn)
rebuild_scopes_bottom_up(spn)
```

The p parameter indicates the probabilities, and the scope indicates the variable we are modeling.


We can now visualize the SPN using:

```python
from spn.io.Graphics import plot_spn

plot_spn(spn, 'basicspn.png')
```

![](https://github.com/BrandonDu/486_Final_Project/blob/main/src/basicspn.png)

We can also marginalize the above SPN over V0 and only leaving V1 and V2 by doing:

```python
from spn.algorithms.Marginalization import marginalize

spn_marg = marginalize(spn, [1,2])
```
Here, we marginalize all the variables not in [1,2], and create a *NEW* structure that knows nothing about the previous one
nor about the variable 0.

We can use this new spn to do all the operations we are interested in.
```python
plot_spn(spn_marg, 'marginalspn.png')
```
![](https://github.com/BrandonDu/486_Final_Project/blob/main/src/marginalspn.png)

We can also dump the SPN as text:
```python
from spn.io.Text import spn_to_str_equation
spn_txt = spn_to_str_equation(spn_marg)
print(spn_txt)
```
And the output is:
```python
(0.6699999999999999*((Categorical(V1|p=[0.9, 0.1]) * Categorical(V2|p=[0.4, 0.6]))) + 0.264*((Categorical(V1|p=[0.55, 0.45]) * Categorical(V2|p=[0.4, 0.6]))) + 0.066*((Categorical(V1|p=[0.5, 0.5]) * Categorical(V2|p=[0.6, 0.4]))))
```
## Classification

We consider SPN for classification, by learning an SPN from data and then comparing the probabilities for the given classes. In particular, we perform classification via approximate most probable explanation (MPE). 

First, we consider the following problem of classifying points into clusters using the following dataset generated by 5 Gaussians.

![basicspn.png](https://github.com/BrandonDu/486_Final_Project/blob/main/src/Gaussian_Clusters_1.png)


```python
np.random.seed(42)
def generate_data(mu_x, std_x, mu_y, std_y, num_points, label):
    X = np.random.normal(mu_x, std_x, num_points)
    Y = np.random.normal(mu_y, std_y, num_points)
    return np.column_stack((X, Y, np.full(num_points, label)))

clusters = [
        {"mean_x": 1, "std_x": 0.6, "mean_y": 4, "std_y": 1.6, "label": 0},  # Cluster 0
        {"mean_x": -3, "std_x": 0.8, "mean_y": -4, "std_y": 1.7, "label": 1},  # Cluster 1
        {"mean_x": 5, "std_x": 0.6, "mean_y": -8, "std_y": 1.6, "label": 2},  # Cluster 2
        {"mean_x": 8, "std_x": 0.5, "mean_y": 9, "std_y": 1, "label": 3},  # Cluster 3
        {"mean_x": -10, "std_x": 0.7, "mean_y": 1, "std_y": 1.4, "label": 4},  # Cluster 4
    ]
data = np.array(
            [
                generate_data(
                    cluster["mean_x"],
                    cluster["std_x"],
                    cluster["mean_y"],
                    cluster["std_y"],
                    num_points,
                    clusters.index(cluster),
                )
                for cluster in clusters
            ]
        )
train_data = data.reshape(num_clusters * num_points, 3)
```

We then learn an SPN from the training data:

```python
from spn.algorithms.LearningWrappers import learn_parametric, learn_classifier
from spn.structure.leaves.parametric.Parametric import Categorical, Gaussian
from spn.structure.Base import Context
spn_classification = learn_classifier(train_data,
                       Context(parametric_types=[Gaussian, Gaussian, Categorical]).add_domains(train_data),
                       learn_parametric, 2)
```
We model our problem as containing 3 features: 2 Gaussians for each of the coordinates and one Categorical for the label. 
We specify that the label is in column 2 (0-indexed), and we generate a basic SPN, which we show below

![basicspn.png](https://github.com/BrandonDu/486_Final_Project/blob/main/src/Example%201%20SPN.png)

Now, we test the classification with an i.i.d dataset, as well as a rectangular lattice of points that are 0.5 apart. We color the classification and find the accuracy for the first dataset.

```python
num_test_points = 10
test_data = np.array(
    [
        generate_data(
            cluster["mean_x"],
            cluster["std_x"],
            cluster["mean_y"],
            cluster["std_y"],
            num_test_points,
            clusters.index(cluster),
        )
        for cluster in clusters
    ]
).reshape(num_clusters * num_test_points, 3)
test_classification = np.copy(test_data)
test_classification[:, -1] = np.nan
```

As previously stated, we do classification via mpe
```python
classification = mpe(spn_classification, test_classification)
```
We plot and color the results
![](https://github.com/BrandonDu/486_Final_Project/blob/main/src/Classification.png)

As we see, all points are classified correctly. 

Now for a grid, we similar find that the SPN performs extremely well, finding well-defined clusters.
![](https://github.com/BrandonDu/486_Final_Project/blob/main/src/Grid%20Classification%20Example%201.png)

### Utilities

Finally, we have basic utilities.

We may confirm that the SPN is valid (consistent and complete, as discussed in our paper):
```python
from spn.algorithms.Validity import is_valid
print(is_valid(spn))
```
The output indicates that the SPN is valid and there are no debugging error messages:
```python
(True, None)
```

For basic statistics on the structure of the SPN, we can use
```python
from spn.algorithms.Statistics import get_structure_stats
print(get_structure_stats(spn))
```


## Acknowledgments
Much of this code was refactored from SPFlow, which can be found at https://github.com/SPFlow/SPFlow. Unfortunately, the code from the provided link has bugs and errors that we had to fix, but most of the ideas for the implementation of the SPN are inspired from there. 
